from chains.database.prompt_for_database import DB_SYSTEM_PROMPT, RELATED_TABLE_PROMPT
from chains.local_vector_stroe import LocalVectorStore

import argparse
import shutil
import datetime
import json
import os
import random
import time
import uuid

import gradio as gr
import requests

from chains.utils import load_vector_store_list
from fastchat.conversation import Conversation
from fastchat.constants import LOGDIR, WORKER_API_TIMEOUT, ErrorCode
from fastchat.model.model_adapter import get_conversation_template
from fastchat.model.model_registry import model_info
from fastchat.serve.gradio_css import code_highlight_css
from fastchat.utils import (
    build_logger,
    server_error_msg,
    violates_moderation,
    moderation_msg,
    get_window_url_params_js,
)

from chains.database import DEFAULT_DB, DB_CONNECTS, get_conn_collections

from configs.model_config import VECTOR_ROOT_PATH
from configs.sys_config import ERROR_STATUS, OK_STATUS

logger = build_logger("gradio_web_server", "gradio_web_server.log")

headers = {"User-Agent": "chat2anything Client"}

no_change_btn = gr.Button.update()
enable_btn = gr.Button.update(interactive=True)
disable_btn = gr.Button.update(interactive=False)

QA_CHAT = "QA"
DB_CHAT = "DB"

controller_url = None
enable_moderation = False

local_vector_store = LocalVectorStore()
local_vector_store.init_cfg()

# learn_more_md = """
# ### License
# The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.
# """

def set_global_vars(controller_url_, enable_moderation_):
    global controller_url, enable_moderation
    controller_url = controller_url_
    enable_moderation = enable_moderation_


def get_conv_log_filename():
    t = datetime.datetime.now()
    name = os.path.join(LOGDIR, f"{t.year}-{t.month:02d}-{t.day:02d}-conv.json")
    return name


def get_model_list(controller_url):
    ret = requests.post(controller_url + "/refresh_all_workers")
    assert ret.status_code == 200
    ret = requests.post(controller_url + "/list_models")
    models = ret.json()["models"]
    priority = {k: f"___{i:02d}" for i, k in enumerate(model_info)}
    models.sort(key=lambda x: priority.get(x, x))
    logger.info(f"Local models: {models}")
    return models


def load_demo_refresh_model_list(url_params):
    models = get_model_list(controller_url)
    selected_model = models[0] if len(models) > 0 else ""
    if "model" in url_params:
        model = url_params["model"]
        if model in models:
            selected_model = model

    dropdown_update = gr.Dropdown.update(
        choices=models, value=selected_model, visible=True
    )

    state = None
    return (
        state,
        dropdown_update,
        gr.Chatbot.update(visible=True),
        gr.Textbox.update(visible=True),
        gr.Button.update(visible=True),
        gr.Row.update(visible=True),
        gr.Accordion.update(visible=True),
    )


def load_demo_reload_model(url_params, request: gr.Request):
    logger.info(
        f"load_demo_reload_model. ip: {request.client.host}. params: {url_params}"
    )
    return load_demo_refresh_model_list(url_params)


def load_demo_single(models, url_params):
    dropdown_update = gr.Dropdown.update(visible=True)
    if "model" in url_params:
        model = url_params["model"]
        if model in models:
            dropdown_update = gr.Dropdown.update(value=model, visible=True)

    state = None
    return (
        state,
        dropdown_update,
        gr.Chatbot.update(visible=True),
        gr.Textbox.update(visible=True),
        gr.Button.update(visible=True),
        gr.Row.update(visible=True),
        gr.Accordion.update(visible=True),
    )


def load_demo(url_params, request: gr.Request):
    logger.info(f"load_demo. ip: {request.client.host}. params: {url_params}")
    return load_demo_single(models, url_params)


def vote_last_response(state, vote_type, model_selector, request: gr.Request):
    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(time.time(), 4),
            "type": vote_type,
            "model": model_selector,
            "state": state.dict(),
            "ip": request.client.host,
        }
        fout.write(json.dumps(data) + "\n")


def upvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"upvote. ip: {request.client.host}") if request else None;
    vote_last_response(state, "upvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def downvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"downvote. ip: {request.client.host}") if request else None;
    vote_last_response(state, "downvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def flag_last_response(state, model_selector, request: gr.Request):
    logger.info(f"flag. ip: {request.client.host}") if request else None;
    vote_last_response(state, "flag", model_selector, request)
    return ("",) + (disable_btn,) * 3


def regenerate(state, request: gr.Request):
    logger.info(f"regenerate. ip: {request.client.host}") if request else None;
    state.messages[-1][-1] = None
    state.skip_next = False
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5


def change_chat_mode(chat_mode):
    if chat_mode == DB_CHAT:
        return QA_CHAT, gr.update(value=0.7)
    else:
        return DB_CHAT,  gr.update(value=0.0)


def clear_history(model_selector, request: gr.Request):
    logger.info(f"clear_history. ip: {request.client.host}") if request else None;
    state = None;
    status = OK_STATUS.format(status="Change to Connect with LLM <b>%s</b>" % model_selector)

    return (state, [[None, status]], "") + (disable_btn,) * 5


def add_text(state, text, request: gr.Request):
    logger.info(f"add_text. ip: {request.client.host}. len: {len(text)}") if not request else None;
    # first-round: state=None
    # then state= Conversion()
    # TODO: other conversation template
    if state is None:
        state = get_conversation_template("vicuna")
        # state = get_conversation_template("one-shot")

    if len(text) <= 0:
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), "") + (no_change_btn,) * 5
    if enable_moderation:
        flagged = violates_moderation(text)
        if flagged:
            logger.info(f"violate moderation. ip: {request.client.host}. text: {text}")
            state.skip_next = True
            return (state, state.to_gradio_chatbot(), moderation_msg) + (
                no_change_btn,
            ) * 5

    # TODO: 4000
    text = text[:1536]  # Hard cut-off

    # roles[0]: Human
    # roles[1]: Assistant
    state.append_message(state.roles[0], text)
    state.append_message(state.roles[1], None)
    state.skip_next = False
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5


def post_process_code(code):
    sep = "\n```"
    if sep in code:
        blocvector_store = code.split(sep)
        if len(blocvector_store) % 2 == 1:
            for i in range(1, len(blocvector_store), 2):
                blocvector_store[i] = blocvector_store[i].replace("\\_", "_")
        code = sep.join(blocvector_store)
    return code


def openai_api_stream_iter(model_name, messages, temperature, top_p, max_new_tokens):
    from configs.opanai_config import init_openai
    init_openai()
    import openai

    logger.info(f"### openai key: {os.environ.get('OPENAI_API_KEY')}, openai url: {os.environ.get('OPENAI_API_BASE')}")

    # Make requests
    gen_params = {
        "model": model_name,
        "prompt": messages,
        "temperature": temperature,
        "top_p": top_p,
    }
    logger.info(f"==== request ====\n{gen_params}")

    res = openai.ChatCompletion.create(
        api_base=os.environ.get("OPENAI_API_BASE"),
        api_key=os.environ.get("OPENAI_API_KEY"),
        model=model_name, messages=messages, temperature=temperature, stream=True
    )
    text = ""
    for chunk in res:
        text += chunk["choices"][0]["delta"].get("content", "")
        data = {
            "text": text,
            "error_code": 0,
        }
        yield data


def anthropic_api_stream_iter(model_name, prompt, temperature, top_p, max_new_tokens):
    import anthropic

    c = anthropic.Client(os.environ["ANTHROPIC_API_KEY"])

    # Make requests
    gen_params = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "top_p": top_p,
    }
    logger.info(f"==== request ====\n{gen_params}")

    res = c.completion_stream(
        prompt=prompt,
        stop_sequences=[anthropic.HUMAN_PROMPT],
        max_tokens_to_sample=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        model=model_name,
        stream=True,
    )
    for chunk in res:
        data = {
            "text": chunk["completion"],
            "error_code": 0,
        }
        yield data


def bard_api_stream_iter(state):
    # TODO: we will use the official PaLM 2 API sooner or later,
    # and we will update this function accordingly. So here we just hard code the
    # Bard worker address. It is going to be deprecated anyway.

    # Make requests
    gen_params = {
        "model": "bard",
        "prompt": state.messages,
    }
    logger.info(f"==== request ====\n{gen_params}")

    response = requests.post(
        "http://localhost:18900/chat",
        json={
            "content": state.messages[-2][-1],
            "state": state.session_state,
        },
        stream=False,
        timeout=WORKER_API_TIMEOUT,
    )
    resp_json = response.json()
    state.session_state = resp_json["state"]
    content = resp_json["content"]
    # The Bard Web API does not support streaming yet. Here we have to simulate
    # the streaming behavior by adding some time.sleep().
    pos = 0
    while pos < len(content):
        # This is a fancy way to simulate token generation latency combined
        # with a Poisson process.
        pos += random.randint(1, 5)
        time.sleep(random.expovariate(20))
        data = {
            "text": content[:pos],
            "error_code": 0,
        }
        yield data


def model_worker_stream_iter(
    conv, model_name, worker_addr, prompt, temperature, top_p, max_new_tokens
):
    # Make requests
    gen_params = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "top_p": top_p,
        "max_new_tokens": max_new_tokens,
        "stop": conv.stop_str,
        "stop_token_ids": conv.stop_token_ids,
        "echo": False,
    }
    logger.info(f"==== request ====\n{gen_params}")

    # Stream output
    response = requests.post(
        worker_addr + "/worker_generate_stream",
        headers=headers,
        json=gen_params,
        stream=True,
        timeout=WORKER_API_TIMEOUT,
    )
    for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\0"):
        if chunk:
            data = json.loads(chunk.decode())
            yield data


def change_QA_mode(QA_mode, history):
    if QA_mode == "Knowledge Store":
        # vector_store_setting, new_vector_store_setting, parameter_row
        return gr.update(open=True, visible=True), gr.update(open=True), history
    else:
        return gr.update(open=False, visible=False), gr.update(open=True), history


# selected_vector_store, vector_store_list, new_vector_store_name, files, chatbot
def add_vector_store(new_vector_store_name, vector_store_list, files, local_vec_store, history):
    # pure python list
    # print("###22", type(vector_store_list))
    if new_vector_store_name == "" or new_vector_store_name is None:
        # status = "<div style='color:red;'>ERROR: knowledge store name can not be empty</div>"
        status = ERROR_STATUS.format(status="knowledge store name can not be empty")
        history += [[None, status]]
        return gr.update(visible=True), vector_store_list, gr.update(visible=True, value=""), gr.update(
            visible=True), history

    if new_vector_store_name in vector_store_list:
        # status = "<div style='color:red;'>ERROR: knowledge store name exists, try another name</div>"
        status = ERROR_STATUS.format(status="knowledge store name exists, try another name")
        history += [[None, status]]
        return gr.update(visible=True), vector_store_list, gr.update(visible=True, value=""), gr.update(visible=True), history

    # start to create new knowledge store
    new_vector_store_path = os.path.join(VECTOR_ROOT_PATH, new_vector_store_name)
    file_list = []

    if not os.path.exists(new_vector_store_path):
        os.makedirs(new_vector_store_path)

    if isinstance(files, list):
        for file in files:
            print('# tmp file path:', file.name)
            # e.g., /tmp/gradio/0a6753bb28844a965e6f307b3a67413984250a51/its.csv
            file_name = os.path.split(file.name)[-1]
            dst_path = os.path.join(new_vector_store_path, file_name)
            shutil.move(file.name, dst_path)
            file_list.append(dst_path)
        # self, file_path: str or List[str], vs_path: str or os.PathLike = None, sentence_len=SENTENCE_LEN
        vector_store_path, loaded_files = local_vec_store.init_vector_store(file_list, new_vector_store_path)
    else:
        # TODO: I found all uploaded files (files or directory) are all list type
        pass

    status = OK_STATUS.format(status=f"File {', '.join([os.path.split(f)[-1] for f in loaded_files])} " \
                  f"have beed vectorized to knowledge store {new_vector_store_name}" if len (loaded_files) else "Fail to upload, try again")

    history += [[None, status]]

    # append the new created knowledge store to drop-up list
    vector_store_list.append(os.path.split(new_vector_store_path)[-1])

    return gr.update(visible=True, choices=vector_store_list, value=vector_store_list[-1]), vector_store_list, gr.update(visible=True, value=""), gr.update(
            visible=True), history


def change_model(model_selector, state, history):
    status = OK_STATUS.format(status="Change to Connect with LLM <b>%s</b>" % model_selector)
    history += [[None, status]]

    return model_selector, state, history


def change_vector_store(selected_vector_name, history):
    if selected_vector_name == '' or not selected_vector_name:
        status = ERROR_STATUS.format(status="<b>%s</b> is not a valid knowledge store" % selected_vector_name)
        history += [[None, status]]
        return gr.update(visible=True), history
    else:
        status = OK_STATUS.format(status="Choose to load knowledge store <b>%s</b>" % selected_vector_name)
        history += [[None, status]]
        return gr.update(visible=True), history


def change_conn(conn_selected):
    conn = DB_CONNECTS[conn_selected]
    db_list = conn.get_database_list()

    conn.open_session_with_db(db_list[0])
    table_list = conn.get_usable_table_names()

    return gr.Dropdown.update(choices=db_list, value=db_list[0] if len(db_list) > 0 else "",), \
            gr.Dropdown.update(choices=table_list, value=None, interactive=True)


def change_db(conn_selected, db_selected):
        conn = DB_CONNECTS[conn_selected]
        conn.open_session_with_db(db_selected)
        table_list = conn.get_usable_table_names()

        return gr.Dropdown.update(choices=table_list)


def http_bot(
    state, model_selector, temperature, top_p, max_new_tokens, chat_mode, QA_mode, selected_vector_store, sql_exec_mode, conn_selected, db_selected, db_table_selected, request: gr.Request
):

    if not request:
        logger.warning("request object is not injected")
    else:
        logger.info(f"http_bot. ip: {request.client.host}")

    start_tstamp = time.time()
    model_name = model_selector
    temperature = float(temperature)
    top_p = float(top_p)
    max_new_tokens = int(max_new_tokens)

    if state.skip_next:
        # This generate call is skipped due to invalid inputs
        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5
        return

    # user's raw input (without prompt from vectors)
    user_last_input = state.messages[-2][1]

    if chat_mode == QA_CHAT and QA_mode == "Knowledge Store":
        if selected_vector_store == "Click here to choose":
            status = ERROR_STATUS.format(status="Choose a valid knowledge store")
            state.messages[-1][1] = status
            yield (
                    state,
                    state.to_gradio_chatbot(),
                    disable_btn,
                    disable_btn,
                    disable_btn,
                    enable_btn,
                    enable_btn,
                )
            return
        else:
            prompt_for_docs = local_vector_store.gen_prompt_with_vector(user_last_input,
                                                                           os.path.join(VECTOR_ROOT_PATH,
                                                                                        selected_vector_store))
            state.messages[-2][1] = prompt_for_docs

    if chat_mode == DB_CHAT:
        if sql_exec_mode == "Yes":
            pass

        if sql_exec_mode == "No":
            db_name = db_selected
            table_name_list = db_table_selected
            if len(table_name_list) == 0 or not table_name_list:
                # TODO: get the related tables via sentence embedding
                pass
            else:
                table_info = DB_CONNECTS[conn_selected].get_table_info(table_name_list)
                prompt_for_db = RELATED_TABLE_PROMPT.format(db_name=db_name, table_info=table_info,
                                                            question=user_last_input)
                state.messages[-2][1] = prompt_for_db

    if len(state.messages) == state.offset + 2:
        # First round of conversation
        new_state: Conversation = get_conversation_template(model_name)
        if chat_mode == DB_CHAT:
            new_state.system = DB_SYSTEM_PROMPT.format(database="MySQL")

        # new_state: Conversation = get_conversation_template("one-shot")
        new_state.conv_id = uuid.uuid4().hex
        new_state.model_name = state.model_name or model_selector
        new_state.append_message(new_state.roles[0], state.messages[-2][1])
        new_state.append_message(new_state.roles[1], None)
        state = new_state
        if model_name == "bard":
            state.session_state = {
                "conversation_id": "",
                "response_id": "",
                "choice_id": "",
                "req_id": 0,
            }

    if model_name == "gpt-3.5-turbo" or model_name == "gpt-4":
        prompt = state.to_openai_api_messages()
        stream_iter = openai_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name == "claude-v1":
        prompt = state.get_prompt()
        stream_iter = anthropic_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name == "bard":
        stream_iter = bard_api_stream_iter(state)
    else:
        # Query worker address
        ret = requests.post(
            controller_url + "/get_worker_address", json={"model": model_name}
        )
        worker_addr = ret.json()["address"]
        logger.info(f"model_name: {model_name}, worker_addr: {worker_addr}")

        # No available worker
        if worker_addr == "":
            state.messages[-1][-1] = server_error_msg
            yield (
                state,
                state.to_gradio_chatbot(),
                disable_btn,
                disable_btn,
                disable_btn,
                enable_btn,
                enable_btn,
            )
            return

        # Construct prompt
        conv = state
        if "chatglm" in model_name.lower():
            prompt = list(list(x) for x in conv.messages[conv.offset :])
        else:
            # get_prompt() will combine the system_msg, user_msg, and llm_msg into a string
            # prompt contains a list of user-llm conversations
            prompt = conv.get_prompt()

        stream_iter = model_worker_stream_iter(
            conv, model_name, worker_addr, prompt, temperature, top_p, max_new_tokens
        )

    state.messages[-1][-1] = "▌"
    state.messages[-2][1] = user_last_input

    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5

    try:
        for data in stream_iter:
            if data["error_code"] == 0:
                output = data["text"].strip()
                if "vicuna" in model_name:
                    output = post_process_code(output)
                state.messages[-1][-1] = output + "▌"
                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5
            else:
                output = data["text"] + f"\n\n(error_code: {data['error_code']})"
                state.messages[-1][-1] = output
                yield (state, state.to_gradio_chatbot()) + (
                    disable_btn,
                    disable_btn,
                    disable_btn,
                    enable_btn,
                    enable_btn,
                )
                return
            time.sleep(0.02)
    except requests.exceptions.RequestException as e:
        state.messages[-1][-1] = (
            f"{server_error_msg}\n\n"
            f"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_btn,
            disable_btn,
            enable_btn,
            enable_btn,
        )
        return
    except Exception as e:
        state.messages[-1][-1] = (
            f"{server_error_msg}\n\n"
            f"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_btn,
            disable_btn,
            enable_btn,
            enable_btn,
        )
        return

    state.messages[-1][-1] = state.messages[-1][-1][:-1]
    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5

    finish_tstamp = time.time()
    logger.info(f"{output}")

    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(finish_tstamp, 4),
            "type": "chat",
            "model": model_name,
            "gen_params": {
                "temperature": temperature,
                "top_p": top_p,
                "max_new_tokens": max_new_tokens,
            },
            "start": round(start_tstamp, 4),
            "finish": round(start_tstamp, 4),
            "state": state.dict(),
            "ip": request.client.host if request else None
        }
        fout.write(json.dumps(data) + "\n")


block_css = (
    code_highlight_css
    + """
pre  {
    white-space: pre-wrap;       /* Since CSS 2.1 */
    white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */
    white-space: -pre-wrap;      /* Opera 4-6 */
    white-space: -o-pre-wrap;    /* Opera 7 */
    word-wrap: break-word;       /* Internet Explorer 5.5+ */
}

.svelte-1g805jl {
        font-size: 16px !important;
        font-weight: bold !important;
    }
 
"""
)


def refresh__vector_store_list():
    vector_store_list = load_vector_store_list(VECTOR_ROOT_PATH)
    return gr.Dropdown.update(choices=vector_store_list)


def build_single_model_ui(models):
    notice_markdown = """
                            # 🤖 Chat2Anything
                            """

    model_description_md = "### An LLM-based tool to deal with your domain knowledge"

    state = gr.State()

    vector_store_list = load_vector_store_list(VECTOR_ROOT_PATH)
    vector_store_list = gr.State(vector_store_list)

    conn_list = list(DB_CONNECTS.keys())
    default_conn = DB_CONNECTS[conn_list[0]]
    db_list = default_conn.get_database_list()

    default_conn.open_session_with_db(db_list[0])
    default_table_list = default_conn.get_usable_table_names()

    CHAT_MODE = gr.State(QA_CHAT)

    gr.Markdown(notice_markdown + model_description_md, elem_id="notice_markdown")

    with gr.Row(elem_id="model_selector_row"):
        model_selector = gr.Dropdown(
            choices=models,
            value=models[0] if len(models) > 0 else "",
            interactive=True,
            label="Backend LLM:"
        ).style(container=False)

    with gr.Row():
        with gr.Column(scale=10):
            chatbot = gr.Chatbot(value=[[None, OK_STATUS.format(
                status=f"Connect with <b>LLM ({model_selector.value})</b> successfully</div>")], ],
                                 elem_id="chatbot", label="Scroll down and start chatting", visible=False
                                 ).style(height=615)

        with gr.Column(scale=5):
            with gr.Tab("Knowledge Chat") as default_tab:
                QA_mode = gr.Radio(["Knowledge Store", "Vanila LLM"],
                                     label="Chat with", value="Knowledge Store")

                with gr.Accordion("Knowledge Stores", open=True) as vector_store_setting:
                    selected_vector_store = gr.Dropdown(choices=vector_store_list.value, label="Choose to load:",
                                                        interactive=True, value=vector_store_list.value[0] if len(
                            vector_store_list.value) > 0 else None)

                    refresh_btn = gr.Button("refresh")

            with gr.Tab("DB Chat") as db_tab:

                with gr.Row():
                    conn_selected = gr.Dropdown(
                        choices=conn_list,
                        value=conn_list[0] if len(conn_list) > 0 else "",
                        interactive=True,
                        label="Connection:"
                    ).style(container=False)
                with gr.Row():
                    db_selected = gr.Dropdown(
                        choices=db_list,
                        value=db_list[0] if len(db_list) > 0 else "",
                        interactive=True,
                        label="Database:"
                    ).style(container=False)
                with gr.Row():
                    db_table_selected = gr.Dropdown(
                        choices=default_table_list,
                        value=None,
                        interactive=True,
                        multiselect=True,
                        label="Table:"
                    ).style(container=False)
                with gr.Row():
                    sql_exec_mode = gr.Radio(["Yes", "No"],
                                        label="Directly Execute SQL", value="No", interactive=False)

            with gr.Accordion("Parameters", open=True) as parameter_row:
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.7,
                    step=0.1,
                    interactive=True,
                    label="Temperature",
                )
                top_p = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=1.0,
                    step=0.1,
                    interactive=True,
                    label="Top P",
                )
                max_output_tokens = gr.Slider(
                    minimum=0,
                    maximum=1024,
                    value=512,
                    step=64,
                    interactive=True,
                    label="Max output tokens",
            )

    with gr.Row() as input_text_row:
        with gr.Column(scale=20):
            textbox = gr.Textbox(
                show_label=False,
                placeholder="Press ENTER to send",
                visible=False,
            ).style(container=False)
        with gr.Column(scale=2, min_width=50):
            send_btn = gr.Button(value="Send", visible=False)

    with gr.Row(visible=False) as button_row:
        upvote_btn = gr.Button(value="👍  Upvote", interactive=False)
        downvote_btn = gr.Button(value="👎  Downvote", interactive=False)
        flag_btn = gr.Button(value="⚠️  Flag", interactive=False)
        regenerate_btn = gr.Button(value="🔄  Regenerate", interactive=False)
        clear_btn = gr.Button(value="🗑️  Clear history", interactive=False)

    # Register listeners
    btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]

    model_selector.change(clear_history, None, [state, chatbot, textbox] + btn_list).then(
        change_model, [model_selector, state, chatbot], [model_selector, state, chatbot]
    )

    selected_vector_store.change(fn=change_vector_store, inputs=[selected_vector_store, chatbot],
                                 outputs=[selected_vector_store, chatbot])

    refresh_btn.click(fn=refresh__vector_store_list, inputs=None, outputs=selected_vector_store)

    QA_mode.change(fn=change_QA_mode,
                        inputs=[QA_mode, chatbot],
                        outputs=[vector_store_setting, parameter_row, chatbot])

    default_tab.select(change_chat_mode, [CHAT_MODE, ], [CHAT_MODE, temperature]).then(clear_history, [model_selector], [state, chatbot, textbox] + btn_list)
    db_tab.select(change_chat_mode, [CHAT_MODE, ], [CHAT_MODE, temperature]).then(clear_history, [model_selector], [state, chatbot, textbox] + btn_list)

    conn_selected.change(fn=change_conn, inputs=conn_selected, outputs=[db_selected, db_table_selected])

    db_selected.change(fn=change_db, inputs=[conn_selected, db_selected], outputs=db_table_selected)

    upvote_btn.click(
        upvote_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )

    downvote_btn.click(
        downvote_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )

    flag_btn.click(
        flag_last_response,
        [state, model_selector],
        [textbox, upvote_btn, downvote_btn, flag_btn],
    )

    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(
        http_bot,
        [state, model_selector, temperature, top_p, max_output_tokens, CHAT_MODE, QA_mode, selected_vector_store, sql_exec_mode, conn_selected, db_selected, db_table_selected],
        [state, chatbot] + btn_list,
    )

    clear_btn.click(clear_history, None, [state, chatbot, textbox] + btn_list)

    textbox.submit(
        add_text, [state, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        # yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5
        http_bot,
        [state, model_selector, temperature, top_p, max_output_tokens, CHAT_MODE, QA_mode, selected_vector_store, sql_exec_mode, conn_selected, db_selected, db_table_selected],
        [state, chatbot] + btn_list,
    )

    send_btn.click(
        add_text, [state, textbox], [state, chatbot, textbox] + btn_list
    ).then(
        http_bot,
        [state, model_selector, temperature, top_p, max_output_tokens, CHAT_MODE, QA_mode, selected_vector_store, sql_exec_mode, conn_selected, db_selected, db_table_selected],
        [state, chatbot] + btn_list,
    )

    return state, model_selector, chatbot, textbox, send_btn, button_row, parameter_row


def build_demo(models):
    with gr.Blocks(
        title="Chat with Your Docs and Database",
        # theme=gr.themes.Base(),
        theme=gr.themes.Soft(),
        css=block_css,
    ) as demo:
        url_params = gr.JSON(visible=False)

        (
            state,
            model_selector,
            chatbot,
            textbox,
            send_btn,
            button_row,
            parameter_row,
        ) = build_single_model_ui(models)

        if args.model_list_mode == "once":
            demo.load(
                load_demo,
                [url_params],
                [
                    state,
                    model_selector,
                    chatbot,
                    textbox,
                    send_btn,
                    button_row,
                    parameter_row,
                ],
                _js=get_window_url_params_js,
            )
        elif args.model_list_mode == "reload":
            demo.load(
                load_demo_reload_model,
                [url_params],
                [
                    state,
                    model_selector,
                    chatbot,
                    textbox,
                    send_btn,
                    button_row,
                    parameter_row,
                ],
                _js=get_window_url_params_js,
            )
        else:
            raise ValueError(f"Unknown model list mode: {args.model_list_mode}")

    return demo


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="127.0.0.1")
    parser.add_argument("--port", type=int)
    parser.add_argument("--controller-url", type=str, default="http://localhost:21001")
    parser.add_argument("--concurrency-count", type=int, default=10)
    parser.add_argument(
        "--model-list-mode",
        type=str,
        default="once",
        choices=["once", "reload"],
        help="Whether to load the model list once or reload the model list every time.",
    )
    parser.add_argument("--share", action="store_true")
    parser.add_argument(
        "--moderate", action="store_true", help="Enable content moderation"
    )
    parser.add_argument(
        "--add-chatgpt",
        action="store_true",
        help="Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)",
    )
    parser.add_argument(
        "--add-claude",
        action="store_true",
        help="Add Anthropic's Claude models (claude-v1)",
    )
    parser.add_argument(
        "--add-bard",
        action="store_true",
        help="Add Google's Bard model",
    )
    args = parser.parse_args()
    logger.info(f"args: {args}")

    set_global_vars(args.controller_url, args.moderate)
    models = get_model_list(args.controller_url)

    if args.add_chatgpt:
        models = models + ["gpt-3.5-turbo", "gpt-4"]
    if args.add_claude:
        models = ["claude-v1"] + models
    if args.add_bard:
        models = ["bard"] + models

    logger.info(f"All Models: {models}")

    demo = build_demo(models)
    demo.queue(
        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False
    ).launch(
        server_name=args.host, server_port=9986, share=args.share, max_threads=200
    )
